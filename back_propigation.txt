starts with concept of chain rule
with optimization problems you have the concept of some function that maps the input X onto the output Y
You then also have an error function that takes the produced Y and actual Y to get an error Z
Then to obtain the updated weights for the function you multiply the derivitive of the function dX/dY by Z
When pytorch tracks the gradients in the computational graph, at each computation step it applies some function to the input to get the output

Back propigation consists of 3 steps
First a forward step where all functions are applied and the loss at all steps is calculated
The second step is computing local gradients or derivitives
The third step is then performing the backward pass using the weights/parameters and by calculating the gradient of the loss at each step while using the chain rule

loss = (WX - Y)^2
first you find the gradient of the prediction or y-hat, and then you find the gradient of the loss
The derivitive of y-hat is found in respect to the weights W
The derivitive of the error S is found in respect to y-hat
walk through steps
S is the error and S^2 is the loss
1. X * W = Y-hat
2. S = (Y-hat - Y)^2
3. dS/dY-hat S^2
4. dY-hat/dW WX
5. dLoss/dW (dS/dY-hat S^2 * dY-hat/dW WX)

Walk through very simple example
W = torch.tensor (1.0, requires_grad = True)
Y = torch.tensor (2.0)
X = torch.tensor (1.0)

Y_hat = X * W
loss = (Y_hat - Y)**2

loss.backward ()
Once loss is computed the backward function does the rest of the gradient calcs and completes back propigation
.backward () performs all of the chain rule derivitives under the hood
